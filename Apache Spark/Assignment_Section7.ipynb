{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLLib and Continuous Application with Batch and Simulated Streaming Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy of this data and its licence are available at https://s3-us-west-2.amazonaws.com/ml-team-public-read/credit-card-fraud.zip\n",
    "\n",
    "Source:  https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/68280419113053/3601578643761083/latest.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.  Read in 'credit-card-fraud/data' as parquet format and save\n",
    "# in variable named data.  Print the schema of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2. Use the count() function on data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3. Display the data with show().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler, VectorSizeHint\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import count, rand, collect_list, explode, struct, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4. Use OneHotEncoderEstimator() with inputCols=[\"amountRange\"], outputCols=[\"amountVect\"].\n",
    "# Save in variable named oneHot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5.  Use VectorAssember() with inputCols=[\"amountVect\", \"pcaVector\"], outputCol=\"features\".\n",
    "# Save in variable named vectorAsember.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6.  Use GBTClassifier() with labelCol=\"label\", featuresCol=\"features\".\n",
    "# Save in variable named estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# When using MLlib with structured streaming, VectorAssembler has \n",
    "# some limitations in a streaming context. Specifically, VectorAssembler \n",
    "# can only work on Vector columns of known size. To address this issue we \n",
    "# can explicitly specify the size of the pcaVector column so that we'll \n",
    "# be be able to use our pipeline with structured streaming. To do this \n",
    "# we'll use the VectorSizeHint transformer.\n",
    "\n",
    "from pyspark.ml.feature import VectorSizeHint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7. Use VectorSizeHint() with inputCol=\"pcaVector\", size=28.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8. Create a Pipeline() and include the stages equal to a \n",
    "# list of oneHot, vectorSizeHint, vectorAssembler, estimator. Save in\n",
    "# a variable named pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# let's split the data into testing and training datasets. \n",
    "# We will shave the test dataset for later\n",
    "#\n",
    "#\n",
    "train = data.filter(col(\"time\") % 10 < 8)\n",
    "test = data.filter(col(\"time\") % 10 >= 8)\n",
    "#\n",
    "# save our data into partitions so we can read them as files\n",
    "#\n",
    "(test.repartition(20).write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(\"test-data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9. Use the count function on the train dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10.  Use the count function on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 11. Fit the train dataset on the pipeline and save\n",
    "# in a variable named pipelineModel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell.\n",
    "# Simulate a stream by reading from a test data file. Typically, you would\n",
    "# use a Kafka cluster and read off Kafka topics.\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 12. Create a schema for the batch data.\n",
    "# Use StructType with four StructFields. Specify the input for each as:\n",
    "# \"time\", IntegerType(), True\n",
    "# \"amountRange\", IntegerType(), True\n",
    "# \"label\", IntegerType(), True\n",
    "# \"pcaVector\", VectorUDT(), True\n",
    "# Save in variable name schema.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 13. Use spark.read and specify the schema, option with \n",
    "# maxFilesPerTrigger with 1 second, and parquet for \"test-data\".\n",
    "# Save in variable named streamingData.\n",
    "\n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 14.  Use transform() on pipelineModel, passing in streamingData.\n",
    "# Save in variable named stream.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 15.  Use pipelineModel.transform() passing in streaming Data.\n",
    "# Use groupBy() with inputs \"label\", \"prediction\".\n",
    "# Use count() with no inputs.\n",
    "# Use sort() with inputs \"label\", \"prediction\".\n",
    "# Save in variable streamPredictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 16.  Convert to pandas dataframe with streamPredictions.toPandas()  \n",
    "# Display output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
