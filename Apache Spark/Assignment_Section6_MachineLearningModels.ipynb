{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification goal is to predict whether the client will \n",
    "# subscribe (Yes/No) to a bank term deposit.\n",
    "# Dataset can be found on Kaggle (Bank Marketing) and UCI\n",
    "# https://www.kaggle.com/rouseguy/bankbalanced/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the Spark Context\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1. Read in the csv data file, bank.csv with header = True and\n",
    "# inferSchema = True. Assign to variable df. Then, use printSchema() on df.\n",
    "\n",
    "# Read in data file \n",
    "\n",
    "# print schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2. Use df.select('deposit') and use the count() to see how\n",
    "# many elements (data points) are there.\n",
    "# Count total number on outcome variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3.  Use df.select() on 'deposit', then groupby() on 'deposit',and then count().\n",
    "# Display the result with show(). You can chain all these operations in 1 line of code.\n",
    "# Check to see if the outcome variable's binary classes are balanced\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4. Take a look at the column names and data types.\n",
    "# Use df.dtypes\n",
    "\n",
    "# take a look at the datatypes on the variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute code in cell  \n",
    "# pull out numeric features from list of datatypes\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
    "\n",
    "# Display pandas dataframe to show columns with numeric features\n",
    "df.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute code in cell below. Wait for output to see visualization \n",
    "# before continuing on to next cell.\n",
    "# Create correlation matrix amongst numeric variables\n",
    "\n",
    "numeric_data = df.select(numeric_features).toPandas()\n",
    "# Use pandas.plotting.scatter_matrix() to check correlations amongst variables\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5.  Execute df.columns.\n",
    "# Take a look at columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6.  Create a new dataframe, df, and select from current dataframe df\n",
    "# 'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', \n",
    "# 'loan', 'contact', 'duration','campaign', 'pdays', 'previous', \n",
    "# 'poutcome', and 'deposit'. \n",
    "# Set new variable name, cols equal to df.columns.\n",
    "# Print schema.\n",
    "\n",
    "# Create a new dataframe, specifying all variables except 'day' and 'month'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set new variable name, cols equal to df.columns.\n",
    "\n",
    "# Print schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data with indexing, encoding, and assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7. Fill in empty slots (empty spaces).\n",
    "# Preprocessing the data for category indexing, one-hot encoding, and \n",
    "# vector assembling.Code is from Databricks\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    \n",
    "    stringIndexer = StringIndexer(inputCol =          , outputCol =         + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[           .getOutputCol()], outputCols=[        + \"classVec\"])\n",
    "    stages += [stringIndexer,        ]\n",
    "    \n",
    "    \n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol =     )\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "\n",
    "numericCols = [     ,      ,      ,       ,       ,      ]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=        , outputCol=\"features\")\n",
    "stages += [        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8. Fill in empty slots (empty spaces)\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages =     )\n",
    "\n",
    "pipelineModel = pipeline.fit(  )\n",
    "\n",
    "df = pipelineModel.      (df)\n",
    "\n",
    "selectedCols = ['label',       ] + cols\n",
    "\n",
    "df = df.select(       )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute code in cell.\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9.  Use randomSplit() on df and split, train, test datasets in\n",
    "# 70/30 splits. Set the seed to 777. Save in variables train, test.\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10. Use LogisticRegression() and set featuresCol to 'features',\n",
    "# labelCol to 'label', and maxIter to 10. Save in variable named lr.\n",
    "# Use fit() on lr, passing in the train dataset. Save in variable named lrModel.\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute code in cell below\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute code in cell below\n",
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 11. use transform() on lrModel by inputting the test dataset. \n",
    "# Save in variable named predictions.\n",
    "# Select 'age', 'job', 'label', 'rawPrediction', 'prediction', 'probability' from predictions.\n",
    "# Display output with show(10).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 12.  Set evaluator equal to BinaryClassificationEvaluator().\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-Boosted Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 13.  Fill in empty slots (empty spaces)\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "\n",
    "gbtModel = gbt.fit(   )\n",
    "\n",
    "predictions = gbtModel.      (test)\n",
    "\n",
    "predictions.select('age', 'job', '     ', '      ', '       ', 'probability').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-Boosted Tree Classifier evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 14.  Set evaluator equal to BinaryClassificationEvaluator()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 15. Fill in empty slots (empty spaces).\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbt.maxBins, [20, 60])\n",
    "             .addGrid(gbt.maxIter, [10, 20])\n",
    "             .build())\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=   , estimatorParamMaps=     ,\n",
    "                     evaluator=    , numFolds=5)\n",
    "\n",
    "# Run cross validations.  This can take about 10 minutes since it is training over 20 trees!\n",
    "cvModel = cv.fit(    )\n",
    "\n",
    "predictions = cvModel.     (test)\n",
    "\n",
    "evaluator.evaluate(        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
