{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the Spark Context\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.4754, 0.5246]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.0949, 0.9051]), prediction=1.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.1066, 0.8934]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.0444, 0.9556]), prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of section on Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|               label|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|  -23.51088409032297| -1.6659388625179559|\n",
      "|(10,[0,1,2,3,4,5,...| -21.432387764165806|  0.3400877302576284|\n",
      "|(10,[0,1,2,3,4,5,...| -12.977848725392104|-0.02335359093652395|\n",
      "|(10,[0,1,2,3,4,5,...| -11.827072996392571|  2.5642684021108417|\n",
      "|(10,[0,1,2,3,4,5,...| -10.945919657782932| -0.1631314487734783|\n",
      "|(10,[0,1,2,3,4,5,...|  -10.58331129986813|   2.517790654691453|\n",
      "|(10,[0,1,2,3,4,5,...| -10.288657252388708| -0.9443474180536754|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.822357870425154|  0.6872889429113783|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.772667465932606|  -1.485408580416465|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.605713514762092|   1.110272909026478|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.544633229269576|  3.0454559778611285|\n",
      "|(10,[0,1,2,3,4,5,...|  -5.055293333055445|  0.6441174575094268|\n",
      "|(10,[0,1,2,3,4,5,...|  -5.039628433467326|  0.9572366607107066|\n",
      "|(10,[0,1,2,3,4,5,...|  -4.937258492902948|  0.2292114538379546|\n",
      "|(10,[0,1,2,3,4,5,...|  -3.741044592262687|   3.343205816009816|\n",
      "|(10,[0,1,2,3,4,5,...|  -3.731112242951253| -2.6826413698701064|\n",
      "|(10,[0,1,2,3,4,5,...|  -2.109441044710089| -2.1930034039595445|\n",
      "|(10,[0,1,2,3,4,5,...| -1.8722161156986976| 0.49547270330052423|\n",
      "|(10,[0,1,2,3,4,5,...| -1.1009750789589774| -0.9441633113006601|\n",
      "|(10,[0,1,2,3,4,5,...|-0.48115211266405217| -0.6756196573079968|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
