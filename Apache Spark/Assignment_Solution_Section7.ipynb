{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLLib and Continuous Application with Batch and Simulated Streaming Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy of this data and its licence are available at https://s3-us-west-2.amazonaws.com/ml-team-public-read/credit-card-fraud.zip\n",
    "\n",
    "Source:  https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/68280419113053/3601578643761083/latest.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: integer (nullable = true)\n",
      " |-- amountRange: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- pcaVector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1.  Read in 'credit-card-fraud/data' as parquet format and save\n",
    "# in variable named data.  Print the schema of data.\n",
    "data = spark.read.parquet(\"credit-card-fraud/data\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284807"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2. Use the count() function on data.\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+--------------------+\n",
      "| time|amountRange|label|           pcaVector|\n",
      "+-----+-----------+-----+--------------------+\n",
      "|52972|          2|    0|[-0.7754608858479...|\n",
      "|41768|          6|    0|[0.87355394100957...|\n",
      "|40769|          7|    0|[0.89089697609461...|\n",
      "|40682|          3|    0|[-0.5729541367324...|\n",
      "|50032|          7|    0|[-2.0530588957594...|\n",
      "|53637|          7|    0|[0.59354926204311...|\n",
      "|39160|          6|    0|[-0.9232352555081...|\n",
      "|52811|          1|    0|[-2.0731024978576...|\n",
      "|44704|          4|    0|[-0.6319386261885...|\n",
      "|53397|          3|    0|[0.08506865775226...|\n",
      "|42761|          5|    0|[-0.7401090585343...|\n",
      "|52826|          7|    0|[-1.0604125919521...|\n",
      "|53371|          3|    0|[1.50340436140654...|\n",
      "|40640|          2|    0|[-2.7843887305120...|\n",
      "|53856|          4|    0|[-1.7521491354721...|\n",
      "|53833|          5|    0|[-1.2246144551417...|\n",
      "|51358|          6|    0|[0.96422665487081...|\n",
      "|40197|          7|    0|[0.05225653867024...|\n",
      "|54985|          6|    0|[1.31441020347478...|\n",
      "|48440|          6|    0|[-1.7396115857888...|\n",
      "+-----+-----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 3. Display the data with show().\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler, VectorSizeHint\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import count, rand, collect_list, explode, struct, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4. Use OneHotEncoderEstimator() with inputCols=[\"amountRange\"], outputCols=[\"amountVect\"].\n",
    "# Save in variable named oneHot.\n",
    "oneHot = OneHotEncoderEstimator(inputCols=[\"amountRange\"], outputCols=[\"amountVect\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5.  Use VectorAssember() with inputCols=[\"amountVect\", \"pcaVector\"], outputCol=\"features\".\n",
    "# Save in variable named vectorAsember.\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"amountVect\", \"pcaVector\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6.  Use GBTClassifier() with labelCol=\"label\", featuresCol=\"features\".\n",
    "# Save in variable named estimator\n",
    "estimator = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# When using MLlib with structured streaming, VectorAssembler has \n",
    "# some limitations in a streaming context. Specifically, VectorAssembler \n",
    "# can only work on Vector columns of known size. To address this issue we \n",
    "# can explicitly specify the size of the pcaVector column so that we'll \n",
    "# be be able to use our pipeline with structured streaming. To do this \n",
    "# we'll use the VectorSizeHint transformer.\n",
    "\n",
    "from pyspark.ml.feature import VectorSizeHint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7. Use VectorSizeHint() with inputCol=\"pcaVector\", size=28.\n",
    "vectorSizeHint = VectorSizeHint(inputCol=\"pcaVector\", size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8. Create a Pipeline() and include the stages equal to a \n",
    "# list of oneHot, vectorSizeHint, vectorAssembler, estimator. Save in\n",
    "# a variable named pipeline.\n",
    "pipeline = Pipeline(stages=[oneHot, vectorSizeHint, vectorAssembler, estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# let's split the data into testing and training datasets. \n",
    "# We will shave the test dataset for later\n",
    "#\n",
    "#\n",
    "train = data.filter(col(\"time\") % 10 < 8)\n",
    "test = data.filter(col(\"time\") % 10 >= 8)\n",
    "#\n",
    "# save our data into partitions so we can read them as files\n",
    "#\n",
    "(test.repartition(20).write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(\"test-data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227570"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 9. Use the count function on the train dataset.\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57236"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 10.  Use the count function on the test dataset.\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 11. Fit the train dataset on the pipeline and save\n",
    "# in a variable named pipelineModel.\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell.\n",
    "# Simulate a stream by reading from a test data file. Typically, you would\n",
    "# use a Kafka cluster and read off Kafka topics.\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 12. Create a schema for the batch data.\n",
    "# Use StructType with four StructFields. Specify the input for each as:\n",
    "# \"time\", IntegerType(), True\n",
    "# \"amountRange\", IntegerType(), True\n",
    "# \"label\", IntegerType(), True\n",
    "# \"pcaVector\", VectorUDT(), True\n",
    "# Save in variable name schema.\n",
    "schema = (StructType([StructField(\"time\", IntegerType(), True), \n",
    "                      StructField(\"amountRange\", IntegerType(), True), \n",
    "                      StructField(\"label\", IntegerType(), True), \n",
    "                      StructField(\"pcaVector\", VectorUDT(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 13. Use spark.read and specify the schema, option with \n",
    "# maxFilesPerTrigger with 1 second, and parquet for \"test-data\".\n",
    "# Save in variable named streamingData.\n",
    "streamingData = (spark.read \n",
    "                 .schema(schema)\n",
    "                 .option(\"maxFilesPerTrigger\", 1)\n",
    "                 .parquet(\"test-data\"))\n",
    "               \n",
    "\n",
    "# Alternative code for structured streaming, notice readStream for streaming data\n",
    "# streamingData = (spark.readStream \n",
    "#                 .schema(schema) \n",
    "#                 .option(\"maxFilesPerTrigger\", 1) \n",
    "#                 .parquet(\"test-data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 14.  Use transform() on pipelineModel, passing in streamingData.\n",
    "# Save in variable named stream.\n",
    "stream = pipelineModel.transform(streamingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 15.  Use pipelineModel.transform() passing in streaming Data.\n",
    "# Use groupBy() with inputs \"label\", \"prediction\".\n",
    "# Use count() with no inputs.\n",
    "# Use sort() with inputs \"label\", \"prediction\".\n",
    "# Save in variable streamPredictions.\n",
    "\n",
    "\n",
    "# Use aggregations groupBy() and sort()\n",
    "streamPredictions = (pipelineModel.transform(streamingData)          \n",
    "          .groupBy(\"label\", \"prediction\")\n",
    "          .count()\n",
    "          .sort(\"label\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  prediction  count\n",
       "0      0         0.0  57132\n",
       "1      0         1.0      8\n",
       "2      1         0.0     18\n",
       "3      1         1.0     78"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 16.  Convert to pandas dataframe with streamPredictions.toPandas()  \n",
    "# Display output\n",
    "streamPredictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
