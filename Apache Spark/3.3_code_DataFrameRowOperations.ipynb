{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first cell, type:\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the second cell, type:\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe, use toDF()\n",
    "df = spark.range(500).toDF(\"number\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "|           18|\n",
      "|           19|\n",
      "|           20|\n",
      "|           21|\n",
      "|           22|\n",
      "|           23|\n",
      "|           24|\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alter values  in rows\n",
    "df.select(df[\"number\"] + 10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     1|\n",
      "|     3|\n",
      "|     5|\n",
      "|     7|\n",
      "|     9|\n",
      "|    11|\n",
      "|    13|\n",
      "|    15|\n",
      "|    17|\n",
      "|    19|\n",
      "|    21|\n",
      "|    23|\n",
      "|    25|\n",
      "|    27|\n",
      "|    29|\n",
      "|    31|\n",
      "|    33|\n",
      "|    35|\n",
      "|    37|\n",
      "|    39|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter row such that number module 2 is not equal to 0.\n",
    "df.filter(df['number'] % 2 != 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use rdd to convert to rdd and perform rdd operations\n",
    "df.rdd.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0),\n",
       " Row(number=1),\n",
       " Row(number=2),\n",
       " Row(number=3),\n",
       " Row(number=4),\n",
       " Row(number=5),\n",
       " Row(number=6),\n",
       " Row(number=7),\n",
       " Row(number=8),\n",
       " Row(number=9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first 10 elements\n",
    "df.rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0),\n",
       " Row(id=1),\n",
       " Row(id=2),\n",
       " Row(id=3),\n",
       " Row(id=4),\n",
       " Row(id=5),\n",
       " Row(id=6),\n",
       " Row(id=7),\n",
       " Row(id=8),\n",
       " Row(id=9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a row object\n",
    "spark.range(10).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a row\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "type(myRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the first element\n",
    "myRow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the third element\n",
    "myRow[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row \n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema =StructType([\n",
    "\tStructField(\"some\", StringType(), True),\n",
    "\tStructField(\"col\", StringType(), True),\n",
    "\tStructField(\"names\", LongType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a row\n",
    "myRow = Row(\"Hello\", None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from a row\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using Row function to create a dataframe\n",
    "from pyspark.sql import Row\n",
    "cats = Row(\"Name\", \"Nickname\", \"Location\", \"Treat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1 = Row('Dakota', 'Sweetie', \"house\", \"salmon\")\n",
    "cat2 = Row('George', 'Grumpy', \"apt\", \"liver\")\n",
    "cat3 = Row('Karrot', 'BiggieK', \"condo\", \"chicken\")\n",
    "cat4 = Row('Tigress', 'Claw', \"street\", \"trout\")\n",
    "cat5 = Row('Kitty', 'Meow', \"house\", \"salmon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Row(Karrot, BiggieK, condo, chicken)>\n"
     ]
    }
   ],
   "source": [
    "print(cat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Row elements\n",
    "shelter1 = Row(id='23456', name='CatColony')\n",
    "shelter2 = Row(id='11111', name='Mauhaus')\n",
    "shelter3 = Row(id='98765', name='BigCatHouse')\n",
    "shelter4 = Row(id='56789', name='WindowCats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='11111', name='Mauhaus')\n"
     ]
    }
   ],
   "source": [
    "print(shelter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Row elements\n",
    "shelterWithCats1 = Row(shelter=shelter1, cats=[cat1, cat2])\n",
    "shelterWithCats2 = Row(shelter=shelter2, cats=[cat3, cat4])\n",
    "shelterWithCats3 = Row(shelter=shelter3, cats=[cat5, cat4, cat1])\n",
    "shelterWithCats4 = Row(shelter=shelter4, cats=[cat2, cat3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelterWithCats = [shelterWithCats1, shelterWithCats2, shelterWithCats3, shelterWithCats4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "dframe = spark.createDataFrame(shelterWithCats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                cats|             shelter|\n",
      "+--------------------+--------------------+\n",
      "|[[Dakota, Sweetie...|  [23456, CatColony]|\n",
      "|[[Karrot, BiggieK...|    [11111, Mauhaus]|\n",
      "|[[Kitty, Meow, ho...|[98765, BigCatHouse]|\n",
      "|[[George, Grumpy,...| [56789, WindowCats]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show dataframe\n",
    "dframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
